# 파이토치 공부
## 10장
### 공부 책 : 딥러닝 파이토치 교과서 (길벗)

#### 임베딩
임베딩 : 사람의 언어를 컴퓨터가 이해할 수 있는 언어 형태 벡터로 변환

1. 희소표현 기반 임베딩
    - 원핫 인코딩 - 주어진 텍스트를 숫자로 변환
        - 단점 : 내적이 0이라 직교(orthogonal) => 관계성 없이 서로 독립적, 차원의 저주
2. 횟수 기반 임베딩
    - Count Vector : 단어를 토큰으로 생성하고 단어 출현 빈도수 이용해 인코딩 해 벡터로 만듦
        - 문서를 토큰 리스트 변환 가능, 문서 토큰 출현 빈도, 문서 인코딩 벡터 변환
    - TF-IDF (TF는 문서 내 특정 단어 출현 빈도, IDF는 특정 단어가 나타난 문서 개수)
        - tf = log(count(t,d)+1)
        - idf = log(전체 문서 개수 / 1+ 특정 단어 t 포함된 문서 개수) <= 분모 0 상황 방지 위해 1을 더해줌
        - 문서 흔한 단어 걸러내거나 특정 단어 중요도 찾을 수 있음

3. 예측 기반 임베딩
    - word2vec : 의미론적 유사한 단어 벡터는 가깝게 표현 (코사인 유사도 이용) => 동의어 찾을 수 있음
        - CBOW : 단어 여러개 나열 후 이와 관련된 단어 추정 (다음 등장 단어 예측)
        - skip-gram : 특정 단어에서 문맥 될 수 있는 단어 예측
    - FastText : 워드투벡터 단점 보완(사전에 없는 단어 얻을 수 없다는 점, 자주 사용되지 않을 시 불안정)
        - n-gram 이용해 단어 분리 결정 <= 사전에 없는 단어 일시 부분 단어와 유사도 계산해 의미 추정
        - n-gram 으로 참고 경우의 수 많아져 자주 사용하지 않더라도 정확도 높음
            - n=1 이면 unigram, n=2 이면 bigram , n=3 이면 trigram

```python
gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg = 0)
# 데이터셋, 최소 빈도 제한, 임베딩된 벡터 차원, 윈도우 크기, sg가 0일 때 cbow, sg 가 1일 때 skip-gram
```

4. 횟수 / 예측 기반 임베딩
    - GloVe : 횟수 기반 LSA와 예측 기반 word2vec 단점 보완 => skip-gram + 통계적 기법

#### 트랜스포머 어텐션
- 어텐션 - 언어 번역에서 사용, 인코더와 디코더 네트워크 사용
    - 모든 벡터 중에서도 꼭 살펴봐야할 벡터들에 집중(attention)
- 트랜스포머 - 어텐션을 극대화 하는 방법
    - 인코더와 디코더를 여러개 중첩시킨 구조, 각각의 인코더와 디코더를 Block 이라 함
    - 인코더 블록 (모든 구조 동일) - 하나의 인코더에는 self-attention 과 feed forward neural network 구성
        - self-attention :  문장에서 각 단어끼리 얼마나 관계가 있는지 계산 반영 => 단어 간 관계 파악 => 전방향 신경망으로 전달
    - 디코더 블록 (세 개의 층)
        - self-attention layer (인코더와 동일)
        - encoder-decoder attention layer (어텐션 매커니즘 수행)
        - 전방향 신경망

- attention score : 현재 디코더 시점 i 단어 예측 위해 인코더 모든 은닉 상태 값 hj 와 현 시점 은닉 상태 si 유사성 판단 값
- attention score => softmax (시간의 가중치) => context vector
- 디코더 은닉 상태 : 어텐션 적용과 미적용된 인코더-디코더 수식 컨텍스트 벡터 (ci 와 c)

1. seq2seq : 번역에 초점, 관계는 중요하지 않고 각 시퀀스 길이 달라도 됨
- sequence labeling : 입력과 출력에 대한 문자열이 같다, 품사 판별에 초점
```python
self.index2word = {0:"SOS",1:"EOS"} # 문장의 시작과 끝
self.n_words = 2 #SOS와 EOS 카운트
if word not in self.word2index:
    self.word2index[word] = self.n_words
    self.word2count[word] = 1
    self.index2word[self.n_words] = word
    self.n_words += 1
else:
    self.word2count[word]+=1

#정규화
sentence = df[lang].str.lower() # 소문자
sentence = sentence.str.replace('[^A-Za-z/s]+',' ') # 알파벳, ? ,! 등 제외 공백 변환
sentence = sentence.str.normalize('NFD') # 유티코드 정규화
sentence = sentence.str.encode('ascii', errors = 'ignore').str.decode('utf-8') # unicode => ascii로 

```
- seq2seq 모델 사용 위해선 인코더와 디코더 정의 해야 함
    - 인코더는 임베딩 계층과 GRU 계층 구성

```python
class Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):
        super(Encoder, self).__init__()
        self.input_dim = input_dim
        self.embbed_dim = embbed_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.embedding = nn.Embedding(input_dim, self.embbed_dim) #임베드 계층 초기화
        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers = self.num_layers) #GRU 계층 초기화
    def forward(self, src):
        embedded = self.embedding(src).view(1,1,-1) # 임베딩 처리
        outputs, hidden = self.gru(embedded) #결과 GRU 모델에 적용
        return outputs, hidden

class Decoder(nn.Module):
    def _init__(self, output_dim, hidden_dim, embbed_dim, num_layers):
        #생략
        self.embedding = nn.Embedding(input_dim, self.embbed_dim) #임베드 계층 초기화
        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers = self.num_layers) #GRU 계층 초기화
        self.out = nn.Linear(self.hidden_dim, output_dim) # 선형 계층 초기화
        self.softmax = nn.LogSoftmax(dim = 1)
    def forward(self, input, hidden):
        input = input.view(1,-1) #(1,배치크기)
        embedded = F.relu(self.embedding(input))
        output,hidden = self.gru(embedded, hidden)
        prediction = self.softmax(self.out(ouput[0]))
        return prediction, hidden

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    def forward(self, input_lang, output_lang, teacher_forcing_ratio = 0.5):
        input_length = input_lang.size(0) #문장 단어 수
        batch_size = output_lang.shape[1]
        target_length = output_lang.shape[0]
        vocab_size = self.decoder.output_dim
        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)

        for i in range(input_length):
            encoder_output, encoder_hidden = self.encoder(input_lang[i])
            #모든 단어 인코딩
        decoder_hidden = encoder_hidden.to(device)
        decoder_input = torch.tensor([SOS_token], device = device)

        for t in range(target_length):
            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)
            outputs[t] = decoder_output
            teacher_force = random.random() < teacher_forcing_ratio
            topv, topi = decoder_output.topk(1)
            input = (output_lang[t] if teacher_force else topi)
            if (teacher_force == False and input.item() == EOS_token):
                break
        return outputs
```
- teacher_force는 seq2seq에서 많이 사용하는 기법으로 목표 단어를 디코더 다음에 입력으로 넣어줌
    - 학습 초기 안정적 훈련, 기울기 빠른 수렴 but 네트워크 불안정
```python
loss += creterion(output[ot], target_tensor[ot])
optimizer = optim.SGM(model.parameters(), lr = 0.01)
criterion = nn.NLLLoss() # 분류문제에서 사용하며 소프트 맥스 사용 명시 해줘야함
```

- seq2seq 모델은 입력 문장 긴 시퀀스 일 시 정확한 처리 어렵다
    - 정보의 손실 발생, 기울기 소멸 문제 발생
- => attention mechanism 등장
    - 특정 시점마다 다른 컨텍스트 벡터 사용
    - 시퀀스가 길수록 성능 향상에 좋음

- 어텐션이 적용된 디코더를 이용해 모델 학습 훈련
```python
self.embedding = nn.Embedding(self.output_size, self.hidden_size)
self.attn = nn.Linear(self.hidden_size*2, self.max_length)
self.attn_combine = nn.Linear(self.hidden_size*2, self.hidden_size)
self.dropout = nn.Dropout(self.droupout_p)
self.gru = nn.GRU(self.hidden_size, self.hiddensize)
self.out = nn.Linear(self.hidden_size, self.output_size)

def forward(self, input, hidden, encoder_outputs):
    embedded = self.embedding(input).view(1,1,-1)
    embedded = self.dropout(embedded)
    attn_weights = F.softmax(
        self.attn(torch.cat((embedded[0], hidden[0]),1)), dim = 1
        )
    attnapplied = torch.bmm(attn_weights, unsqueeze(0), encoder_outputs.unsqueeze(0))

    output = torch.cat((embedded[0], attn_applied[0]),1)
    output = self.attn_combine(output).unsqueeze(0)

    output = F.relu(output)
    out, hidden = self.gru(output, hidden)
    output = F.logsoftmax(self.out(output[0]), dim = 1)
    return output, hidden, attn,weights
```

#### BERT
- 양방향 자연어 처리 모델
- 트랜스포머 이용해 구현되었으며 방대한 양의 텍스트 데이터로 사전 훈련된 언어 모델
- transformer 인코더를 쌓아올린 구조
- 문장 예측 (Next Sentence Prediction, NSP)에 사용
1. 문장 시작은 [CLS], 문장 끝은 [SEP]
2. 한 문장 단어 토큰화 (ex 강아지 => 강##, #아#, ##지)
3. 각 토큰들에 고유 아이디 부여 (없을 시 0)

- 전이 학습 기반 (인코더-디코더)
    - 기존 (CNN, RNN)과 달리 어텐션 개념 도입
    - 인코더만 사용

```python
from pytorch_transformer import BertTokenizer, BertForSequenceClassification

for text, label in train_loader:
    optimizer.zero_grad()
    encoded_list = [tokenizer.encode(t, add_special_tokens = True) for t in text]
    padded_list = [e + [0]*(512-len(e)) for e in encoded_list] #제로패딩 적용
    sample = torch.tensor(padded_list)
    sample, label = sample.to(device), label.to(device)
    labels = torch.tensor(label)
    outputs = model(sample, labels = labels)
    loss, logits = outputs

    pred = torch.argmax(F.softmax(logits), dim = 1)
    correct = pred.eq(labels)
    total_correct += correct.sum().item()
    total_len += len(labels)
    running_loss += loss.item()
    loss.backward()
    optimizer.step()
    global_step +=1
```

#### 한국어 임베딩
- `bert-base-multilingual-cased` 사용
```python
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
marked_text = '[CLS]' + text + '[SEP]'
tokenized_text = tokenizer.tokenize(marked_text)
# 정확하지 않을 수 있으므로 KoBert 모델도 유용

model = BertModel.from_pretrained('bert-base-multilingual-cased', output_hidden_states = True)
# 임베딩 계층 포함 13계층으로 구성, 은닉상태 값 가져옴
model.eval() # 평가 모드

torch.squeeze(token_embeddings, dim = 1) # 배치차원 1 제거
token_embeddings.permute(1,0,2) # 차원을 맞교환

from scipy.spatial.distance import cosine
1-cosine(벡터, 벡터) # 단어 사이의 코사인 유사도 계산
```
- 한국어의 경우 정확도는 KoBert 모델이 더 높음