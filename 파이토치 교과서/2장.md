# 파이토치 공부
## 2장
### 공부 책 : 딥러닝 파이토치 교과서 (길벗)

#### 파이토치 특징 : GPU에서 텐서 조작 및 동적 신경망 구축 가능한 프레임 워크
- GPU : 연산 속도 빠르게(미분 계산), cuda나 cuDNN API 이용해 GPU 사용 가능, 병렬 연산이기에 속도 CPU보다 빠름
- 텐서 : 다차원 행렬
- 동적 신경망 : 훈련 반복할 때마다 네트워크 변경 가능한 신경망, 모델의 네트워크 조작 가능

#### 파이토치 장점
1. 효율적인 계산 (디버깅이 직관적, 간결)
2. 낮은 CPU 활용 (학습 및 추론 속도 빠름)
3. 직관적 인터페이스

#### 아키텍처
1. API
- torch.autograd (자동 미분)
- torch.nn (신경망 쉽게 구축, 사용) - 특히 CNN, RNN, 정규화 등 포함 되어 쉽게 구축 학습
- torch.multiprocessing (텐서의 메모리 공유 가능)
- torch.utils

2. 파이토치 엔진
- Autograd C++ - 가중치, 바이어스 업데이트 과정 필요 미분 자동 계산
- Aten C++ - c++ 텐서 라이브러리 제공
- JIT C++ - 계산 최적화 컴파일러 (애플리케이션 속도 결정하는데 핵심적 역할)

3. 연산 처리
- C나 CUDA 패키지는 상위 API 할당된 거의 모든 계산 수행

---
- torch.FloatTensor : 32비트 부동 소수점
- torch.DoubleTensor : 64비트 부동 소수점
- torch.LongTensor : 64비트 부호 있는 정수
`텐서 간 타입 다를 시 연산 불가능`

#### 차원 조작
1. 차원 변경 - view
2. 텐서 결합 - stack, cat
3. 차원 교환 - t, transpose

#### 모델 정의
1. 계층(layer) : 모듈 구성하는 한 개의 계층으로 합성곱층, 선형 계층 등
2. 모듈 : 한 개 이상의 계층이 모여 구성, 여러 모듈을 통해 새로운 모듈 생성 가능
3. 모델 : 최종적 원하는 네트워크

- nn.Module()
    - __ init__() : 모델에서 사용될 모듈, 활성화 함수 등 정의 
    - forward() : 모델 실행 되어야하는 연산 정의
- nn.Sequential 사용 시 포함된 각 모듈 순차적 실행, 모델 계층 복잡할 수록 효과 뛰어남
- model.modules() : 모든 노드 반환
- model.children() : 같은 수준의 하위 노드 반환

#### 모델 파라미터 정의
- 손실 함수 : 학습 출력과 실제 값 사이 오차, 모델의 정확성 측정
    - BCELoss : 이진분류
    - CrossEntropyLoss : 다중 클래스 분류
    - MSELoss : 회귀 모델
- 옵티마이저 : 데이터와 손실 함수 바탕 업데이트 방법 결정
    - step() 을 통해 파라미터 업데이트
    - zero__grad() 통해 기울기 0으로 초기화
    - torch.optim.lr_scheduler : 에포크에 따라 학습률 조절
        - 전역 최소점 근처에 다다르면 학습률 줄여 최적점 찾아갈 수 있도록 함
        - optim.lr_scheduler.LambdaLR : 람다 함수 이용
        - optim.lr_scheduler.StepLR : 특정 step 마다 감마 비율 만큼 감소
        - optim.lr_scheduler.MultiStepLR : 지정된 에포크에만 감마 비율만큼 감소
        - optim.lr_scheduler.ExponentialLR : 에포크마다 이전 학습률에 감마만큼 곱
        - optim.lr_scheduler.CosineAnnealingLR : 코사인 함수 형태처럼 변경
        - optim.lr_scheduler.ReduceLR0nPlateau : 동적으로 학습률 변화
#### 모델 학습
```python
for epoch in range(1,101):
    y_hat = model(x_train)
    loss = criterion(y_jhat, y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```
1. 모델, 손실함수, 옵티마이저 정의
2. zero_grad() 사용해 초기화
3. backward() 사용해 역전파 학습 (중첩이 되므로 RNN 아닐 시 zero_grad로 초기화)
4. step을 통해 기울기 업데이트

#### 모델 평가
```python
a = torchmetrics.Accuracy() # 정확도 모델 평가 초기화
b = a(predict, target) # 현 배치 사이즈에서 모델 평가
c = a.compute() # 모든 배치에서 모델 평가
```
이 외에도 confusion_matrix, accuracy_score, classification_report

#### 훈련 과정 모니터링
1. 텐서보드 설정 (set up)
2. 텐서보드에 기록
3. 사용하여 모델 구조 모니터링
```python
from torch.utils.tensorboard import SummaryWriter
write = SummaryWriter(위치)
writer.add_scalar(값들)
writer.close()
```
```python
model.eval() # 평가 시 모든 노드 사용 의미로 검증과 테스트 데이터 셋 사용
with torch.no_grad():
```
- 파이토치는 모든 연산과 기울기 값을 저장하지만 검증 과정에서는 역전파 필요하지 않기에 with torch.no_grad() 사용하여 기울기 값 저장하지 않도록 함

#### 실습
- 범주형 데이터 텐서 변환 방법
범주형 데이터 -> 카테고리형 -> 넘파이 배열 -> 텐서
    - 넘파이 배열 변경을 위해서는 cat.codes.values를 사용 but 어떤 숫자로 매핑 되어 있는지 확인 어려움
    - np.stack으로 객체 합치기
    - torch.tensor(데이터, dtype) 으로 텐서화 시켜주기
```python
np.concatenate(axis = 1) # 열을 늘려줌
np.concatenate(axis = 0) # 행을 늘려줌
np.stack(axis = 1) # 차원이 동일해야하며 1차원은 2차원으로 2차원은 3차원으로 만들어줌
```
- ravel(), reshape(), flatten() 사용하여 텐서 차원 변경

```python
# 네트워크 생성
class Model(nn.Module): # 클래스 형태 구현 모델 상속
    def __init__(self, embeding_size, output_size, layers, p = 0.4): # 모델에 사용될 파라미터와 신경망 초기와 용도
        super().__init__()
        self.all_embedings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])
        self.embedding_dropout = nn.Dropout(p)
        all_layer = []
        num_categorical_cols = sum((nf for ni, nf in embedding_size))
        input_size = num_categorical_cols #입층력 크기 찾기 위해

        for i in layers:
            all_layers.append(nn.Linear(input_size, i)) #선형 변환
            all_layers.append(nn.ReLU(inplace = True)) #활성화 함수
            all_layers.append(nn.BatchNorm1d(i)) # 배치 정규화 (데이터 평균과 분산 조정)
            all_layers.append(nn.Dropout(p)) # 과적합 방지용
            input_size = i
        
        all_layers.append(nn.Linear(layers[-1], output_size))
        self.layers = nn.Sequential(*all_layers) #Sequential를 사용해 모든 계층이 순차적 실행이 되도록 함
    def forward(self, x_categorical):
        embeddings = []
        for i,e in enumerate(self.all_embeddings):
            embeddings.append(e(x_categorical[:,i]))
            x = torch.cat(embeddings,1)
            x = self.embedding_dropout(x)
            x = self.layers(x)
            return x
model = Model(categorical_embedding_sizes, 4, [200,100,50], p = 0.4)
# 파라미터 정의
loss_function = nn.CrossEntropyLoss() # 다중 분류이므로 크로스 엔트로피 손실 함수
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)

#모델 학습
epochs = 500
losses = []
train_output = train_output.to(device = torch.device('cuda'), dtype = torch.int64) #cuda를 이용해 GPU 사용
for i in range(epochs): #각 반복마다 손실 함수가 오차 계산
    i +=1
    y_pred = model(categorical_train_data)
    single_loss = loss_function(y_pred, train_output)
    losses.append(single_loss)
    
    if i% 25==1:
        print(f'epoch: {i:3} loss : {single_loss.item():10.8f}')
    optimizer.zero_grad()
    single_loss.backward()
    optimizer.step()
print(f'epoch: {i:3} loss : {single_loss.item():10.10f}')

# 모델 예측
test_outputs = test_outputs.to(device = torch.device('cuda'), dtype = torch.int64)
with torch.no_grad():
    y_val = model(categorical_test_data)
    loss = loss_function(y_val, test_outputs) #훈련 데이터 셋과 손실 값 비슷하면 과적합 발생하지 않은 것임

# 정확도 확인
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
# 모두 (test_outputs, y_val) 넣어야 됨
```
#### 모델 평가
1. 정확도 : 전체 예측 건수에서 정답을 맞힌 건수의 비율
2. 재현율 : 실제로 정답이 1일시 1로 예측한 비율
3. 정밀도 : 1이라고 예측한 것 중 실제로 정답 1인 비율
4. f1 스코어 : 정밀도와 재현율의 조화평균