# 파이토치 공부
## 4장
### 공부 책 : 딥러닝 파이토치 교과서 (길벗)

#### 용어
1. 층
- 입력층 : 데이터 받아들이는 층
- 은닉층 : 모든 입력 노드로부터 입력 값 받아 가중 합 계산하고 활성화 함수 적용해 출력층에 전달
- 출력층 : 신경망 최종 결과값 포함

2. 가중치 : 노드와 노드 간 연결 강도
- 입력 값의 연산 결과 조정 역할

3. 바이어스 : 하나의 뉴런에서 활성화 함수를 거쳐 최종 출력되는 값 조절 역할

4. 가중합, 잔달 함수 : 가중치와 신호의 곱

5. 함수
- 활성화 함수 : 신호를 입력받아 처리 출력
    - 시그모이드 함수 : 결과를 0~1 사이 비선형 형태로 변형
        - 주로 로지스틱 같이 분류에 사용
        - 기울기 소멸 문제 발생하여 딥러닝에선 잘 사용 안함
    - 하이퍼볼릭 탄젠트 : 결과를 -1~1 사이 비선형 형태로 변형
        - 기울기 소멸 문제 여전히 발생
    - 렐루 함수 : 음수 일 때는 0, 양수일 때는 x 출력
        - 경사하강법에 영향을 주지 않아 학습 속도 빠르고, 기울기 소멸 문제 발생 안함
        - 음수에 0을 출력하기에 능력이 감수 -> 리키 렐루 함수 사용
        - torch.nn.ReLu
    - 리키 렐루 함수 : 입력 값 음수 일 시 0.00001 같이 매우 작은 수 반환
    - 소프트 맥스 함수 : 0~1 사이 출력, 총합 항상 1
        - 출력 노드 활성화 함수로 많이 사용
        - torch.nn.Softmax

- 손실 함수 : 가중치 학습 위해 출력 함수 결과와 실제 값 오차를 측정하는 함수
    - MSE : 값이 작을수록 예측력이 좋다
        - 회귀에서 손실 함수로 주로 사용
        - torch.nn.MSELoss
    - Cross Entorpy Error (CEE) : 분류 문제에서 원핫인코딩 했을 때만 사용하는 오차 계산법
        - 자연로그를 취해 지역최소에서 멈추는 것 방지

#### 학습
1. 순전파 : 모든 뉴런이 이전 층의 뉴런에서 수신한 정보에 변환 적용 후 다음층의 뉴런으로 전송
2. 손실함수 : 오차 추정
    - 0이 이상적
    - 0에 가깝도록 하기 위해 모델 훈련 반복해 가중치 조정
3. 역전파 : 오차 계산 후 정보를 역으로 전파
    - 다시 가중치 값에 따라 결과 반복

#### 문제와 해결 방안
1. 과적합 : 훈련 데이터 과하게 학습 -> 오차 감소하지만 검증 시 오차 증가
- 드롭 아웃으로 해결 : 학습 과정 중 일부 노드 학습 제외
- torch.nn.Dropout(퍼센트)
2. 기울기 소멸 : 학습 양 0에 가까워져 오차 줄이지 못하게 수렴
- 렐루 사용
3. 성능 나빠짐 : 경사하강법 반복 과정 통해 성능 나빠짐
- 확률적 경사 하강법 : 임의 선택 데이터에 대해 기울기 계산 -> 정확도 낮을 수 있으나 속도 빠름
- 배치 경사 하강법 : 전체 훈련 셋에 가중치 편미분, 한 스텝에 모든 훈련 셋 사용해 학습 오래 걸림
- 미니 배치 경사하강법 : 전체 셋을 미니 배치 여러 개로 나눈 후 각 기울기 구한 다음 평균 기울기 구해 모델 업데이트 -> 빠르고 안정적

#### 옵티마이저
1. 아다그라드 : 변수(가중치)의 업데이트 횟수에 따라 학습률 조정
    - 많이 변화한 곳은 정밀하게, 적게 변화한 곳은 빠르게
    - troch.optim.Adagrad
2. 아다델타 : 아다그라드에서 학습이 멈추지 않도록 어느 선에서 끊어줌
    - torch.optim.Adadelta
3. 알엠에스프롭 : 아다그라드에서 값이 커지는 것 방지하고자 학습률 크기 비율 조정
    - torch.optim.RMSprop
4. 모멘텀 : 같은 방향으로 일정한 비율만 수정, 관성 효과 장점
    - 확률적 경사하강법과 같이 사용
    - torch.optim.SGD(momentum = 0.9)
5. 네스테로프 모멘텀 : 모멘텀의 훨씬 멀리 갈 수 있다는 단점을 보완
    - 절반 정도 이동 후 어떤 방식으로 이동해야 하는 지 다시 계산, 빠른 이동 속도, 절적한 시점 제동 용이
    - torch.optim.SGD(momentum = 0.9, newterov = True)
6. 아담 : 모멘텀과 알엠에스프롭 장점 결합
    - 제일 보편적 사용
    - torch.optim..Adam

#### 알고리즘
1. 심층 신경망 (DNN)
    - 다수의 은닉층 추가하여 별도의 트릭 없이 비선형 분류 가능
    - 연산량 많고 기울기 소멸 문제 발생 -> 드롭아웃, 렐루, 배치 정규화 적용
2. 합성곱 신경망 (CNN) : 합성곱층과 풀링층 포함
    - 이미지에서 객체 탐생 및 위치 찾는데 유용
    - 각 층 입출력 형상 유지
    - 이미지 공간 정보 유지하면서 인접 이미지와 차이 있는 특정 인식
    - 복수 필터로 이미지 특징 추출, 학습
    - 필터가 있어 학습 파라미터 적음
3. 순환신경망 (RNN) : 시계열
    - 시간성 정보 이용해 데이터 특징 잘 다룸
    - 시간에 따라 내용 변하므로 동적, 가변적
    - 기울기 소멸 문제 -> LSTM
4. 제한된 볼츠만 머신 : 가시층과 은닉층만 연결
    - 차원 감소, 분류, 선형 회귀, 협업 필터링, 특성 값 학습, 주제 모델링
    - 기울기 소멸 문제 해결 위해 사전 학습 용도로 활용
    - 심층 신뢰 신경망 (DBN) 요소로 활용
5. 심층 신뢰 신경망 : 사전 훈련된 제한된 볼츠만 머신을 층층이 쌓아올린 구조
    - 레이블 없는 데이터에 대해서도 비지도 학습 가능
    - 계층적 구조 생성
    - 위로 갈 수록 추상적 특성 추출
    - 다층 퍼셉트론의 가중치 초깃값 사용
