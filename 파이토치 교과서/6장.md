# 파이토치 공부
## 6장
### 공부 책 : 딥러닝 파이토치 교과서 (길벗)

#### LeNet-5
- LeNet-5 : 합성곱과 다운샘플링(혹은 풀링)을 반복적 거치면서 완전연결층에서 분류를 수행
```python
def __init__(self, file_list, transform = None, phase = 'train'):
    self.file_list  = file_list
    self.transform = transform # ImageTrnasform(size, mean,std) 전처리 작업
    self.phase = phase # 이곳에 train 또는 val, test를 넣을 수 있음
class LeNet(nn.Module):
    def __init__(self):
        super(LeNet,self).__init__()
        self.cnn1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 5, stride = 1, padding = 0)
        # 입력 형태는 (3,224,224) 출력 형태는 (16,220,220)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)
        #최대 풀링 적용. 출력 220/2 => (16,110,110)
        self.cnn2 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 5, stride = 1, padding = 0)
        #출력 형태 (32,106,106)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(kernel_size = 2)
        #최대 풀링 적용. 출력 형태 (32,53,53)
        self.fc1 = nn.Linear(32*53*53)
        self.relu3 = nn.ReLU()
        self.fc2 = nn.Linear(512,2)
        self.output = nn.Softmax(dim = 1)
    def forward(self, x):
        out = self.cnn1(x)
        out = self.relu1(out)
        out = self.maxpool1(out)
        out = self.cnn2(x)
        out = self.relu2(out)
        out = self.maxpool2(out)
        out = out.view(out.size(0),-1)
        out = self.fc1(out)
        out = self.fc2(out)
        out = self.output(out)
        return out
```
- conv2d 출력크기 구하는 공식 : (W-F+2P)/S +1
    - W:입력 데이터 크기
    - F : 커널 크기
    - P : 패딩 크기
    - S : 스트라이드
- torchsummary 라이브러리 통해 네트워크 구조 확인 가능

```python
optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)
# 모멘텀 SGD는 SGD에 관성 추가된 것
criterion = nn.CrossEntropyLoss()

def train_model(model, dataloader_dict, criterion, optimizer, num_epoch):    
    since = time.time()
    best_acc = 0.0
    
    for epoch in range(num_epoch):
        print('Epoch {}/{}'.format(epoch + 1, num_epoch))
        print('-'*20)
        
        for phase in ['train', 'val']: #훈련 셋과 검증 셋을 분리해서 훈련셋일 경우 학습시킨다     
            if phase == 'train':
                model.train()
            else:
                model.eval()
                
            epoch_loss = 0.0
            epoch_corrects = 0
            
            for inputs, labels in tqdm(dataloader_dict[phase]):
                inputs = inputs.to(device)
                labels = labels.to(device)
                optimizer.zero_grad()
                
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                        
                    epoch_loss += loss.item() * inputs.size(0)
                    epoch_corrects += torch.sum(preds == labels.data)
                    
            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)
            #최종 오차(오차를 데이터셋 길이로 나눔)
            epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)
            #최종 정확도(맞은 것을 데이터 셋 길이로 나눔)
            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))
            
            if phase == 'val' and epoch_acc > best_acc: #검증 셋에 대한 최적 정확도 저장
                best_acc = epoch_acc
                best_model_wts = model.state_dict()
                
    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))
    return model
#모델 테스트 함수
id_list = []
pred_list = []
_id=0
with torch.no_grad():
    for test_path in tqdm(test_images_filepaths):
        img = Image.open(test_path)
        _id =test_path.split('/')[-1].split('.')[1] #경로 전처리
        transform = ImageTransform(size, mean, std) #이미지 전처리
        img = transform(img, phase='val') #검증 셋 가져오기
        img = img.unsqueeze(0) #unsqueeze는 차원을 추가해주는 것이기에 뒤에 숫자 0을 넣으면 인덱스 [0] 자리에 차원을 하나 추가해주는 것
        img = img.to(device)

        model.eval()
        outputs = model(img)
        preds = F.softmax(outputs, dim=1)[:, 1].tolist()  
        #각 행(dim =1) 합이 1이 되도록하며 [:,1] 모든 행의 [1]번째 인덱스인 두번째 컬럼을 가져온 후 tolist() 배열 리스트 형태 변환      
        id_list.append(_id)
        pred_list.append(preds[0])
```

#### AlexNet
- 합성곱층 다섯 개와 완전 연결층 세개로 구성 => 완전연결층은 소프트 맥스 활성화 함수 사용(카테고리 1000개 분류)
- GPU 두개를 기반으로 병렬 구조

- 예제에서도 합성곱+활성화함수(ReLU)+풀링 다섯번 반복 후 두개의 완전 연결층과 출력층으로 구성

- nn.AdaptiveAvgPool2d 는 nn.AgvPool2d처럼 풀링을 위해 사용
    -  풀링 작업이 끝날 때 필요한 출력 크기를 정의
    - nn.AvgPool2d는 커널크기, 스트라이드 패딩을 매번 지정해주지만 nn.AdaptiveAgvPool2d는 출력 크기에 대해 조정
- 이전 LeNet과 합성곱과 완전연결층 제외하면 나머지 코드는 거의 동일

#### VGGNet
- 합성곱층 파라미터 수 줄이고 훈련 시간 개선 = 네트워크 깊게 만드는 것이 성능에 어떤 영향 미치는 지 확인 위해 합성곱층 사용 필터 크기를 가장 작은 3x3으로 고정

- 최대 풀링 크기는 2x2에 스트라이드는 2
- 결과적으로 64개의 224x224x64 생성 => 마지막은 렐루가 아닌 소프트맥스 사용
```python
vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']
#8 합성곱, 3 풀링층 = 11 전체 계층
vgg13_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']
#10 합성곱, 3 풀링층 = 13 전체 계층
vgg16_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 
                512, 'M']
#13 합성곱, 3 풀링층 = 16 전체 계층
vgg19_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 
                512, 512, 512, 512, 'M']
#16 합성곱, 3 풀링층 = 19 전체 계층

#VGG 계층 정의
def get_vgg_layers(config, batch_norm):    
    layers = []
    in_channels = 3
    
    for c in config:
        assert c == 'M' or isinstance(c, int)
        # assert 는 뒤의 조건이 True가 아니면 에러를 발생
        # isinstance 는 True/False 확인
        if c == 'M':
            layers += [nn.MaxPool2d(kernel_size = 2)]
        else: #불러온 값이 숫자면 합성곱(Conv2d)
            conv2d = nn.Conv2d(in_channels, c, kernel_size = 3, padding = 1)
            if batch_norm:
                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace = True)]
            else:
                layers += [conv2d, nn.ReLU(inplace = True)]
            in_channels = c
            
    return nn.Sequential(*layers) #네트워크 모든 계층 반환

models.vgg11_bn(pretrained = True) #배치 정규화 적용된 사전 훈련된 VGG11 모델 사용

torchvision.datawets.ImageFolder(path, transform = train_transforms)
# 폴더가 계층적으로 위치해 있는 구조에서 데이터 셋 불러오고 싶을 때 ImageFolder 사용

#정확도 측정
def calculate_accuracy(y_pred, y):
    top_pred = y_pred.argmax(1, keepdim = True) #1은 axis = 1(열)에 가장 큰 값의 인덱스를 찾는다는 의미, 출력 텐서를 입력 텐서와 동일한 크기 유지
    correct = top_pred.eq(y.view_as(top_pred)).sum() #eq는 서로 같은 지 비교
    # y에 대한 텐서 크기를 top_pred의 텐서 크기로 변경
    #합계 구하는 이유는 예측과 정답 일치 개수 합산
    acc = correct.float() / y.shape[0]
    return acc

# 예측 중 정확히 예측한 것 추출
images, labels, probs = get_predictions(model, test_iterator) #예측 확인 함수
pred_labels = torch.argmax(probs, 1)
corrects = torch.eq(labels, pred_labels)
correct_examples = []

for image, label, prob, correct in zip(images, labels, probs, corrects):
    #zip은 여러개를 하나의 튜플로 묶어주는 역할
    if correct:
        correct_examples.append((image, label, prob))

correct_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values) # 데이터 정렬을 key 값으로
#dim =0인 행 기준 max 값을 가져옴

image.clamp_(min = image_min, max = image_max) # 주어진 min, max의 범주에 이미지가 위치하도록 저장

image.permute(1,2,0) # 축을 변경할 때 사용
```

#### GoogLeNet
- 주어진 하드웨어 자원 최대한 효율적 이용하며 학습 능력은 극대화, 인셉션 모듈 추가
     - 인셉션 모듈 : 1x1, 3x3, 5x5 합성곱 연산 각각 수행
     - 3x3 최대 풀링은 입출력 높이와 너비가 같아야하므로 패딩을 추가해줘야함 => 희소연결
        - 희소연결 : 빽빽하게 연결된 신경망 대신 관련성 높은 노드끼리만 연결하는 방법 => 연산량 적어져 과적합 해결
        - 총 네가지 연산을 진행 : 1x1, 1x1 + 3x3, 1x1 + 5x5, 3x3 최대풀링 + 1x1
- 합성곱 신경망에서 과적합, 기울기 소멸 문제, 학습 시간 지연, 연산 속도 문제 발생 -> GoogLeNet으로 문제 해결 가능

#### ResNet
- 신경망 깊이 길어진다고 성능이 좋지는 않음 (일정 단계 다다르면 성능 나빠짐)
- 레지듀얼 블록 도입 : 기울기 잘 전파될 수 있도록 숏컷
    - 합성곱층 여러개를 하나의 블록으로 묶은 것 => 여러개의 레지듀얼 블록 쌓은 것을 ResNet이라 함
- 계층을 계속 쌓으면 파라미터 수가 문제가 됨 => 병목 블록 사용
    - 합성곱 층 앞 뒤로 1x1 합성곱층 붙여 채널 수를 조절하며 차원을 줄였다 늘렸다 함

- 아이덴티티 매핑 (숏컷, 스킵 연결) = 입력 x가 어떤 함수를 통과하더라도 다시 x로 출력
```python
def forward(self, x):        
    i = x        
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)        
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)        
    x = self.conv3(x)
    x = self.bn3(x)
                
    if self.downsample is not None:
        i = self.downsample(i) #다운 샘플링 : 특성 맵크기 줄이기 위해 풀링과 같은 역할
    # 입력과 출력 형태 같아주도록 스트라이드 2를 가진 1x1 합성곱 계층 하나 연결
    x += i #아이덴티티 매핑
    x = self.relu(x) 
    return x
```
- 프로젝션 숏컷 (합성곱 블록) : 입력 및 출력 차원이 동일하지 않고 입력 차원을 출력에 맞추어 변경

- 기본 블록에서는 3x3 합성곱 두개를 갖지만 병목 블록은 1x1, 3x3, 1x1 합성곱층 구조를 가짐
=> 계층을 더 깊게 쌓아 계산에 대한 비용을 줄일 수 있음, 더 많은 비선형성 처리 가능함

- 사전 훈련된 ResNet 모델 : models.resnet50(pretained = True)

#### 객체 인식
- 객체 인식 = 여러가지 객체에 대한 분류 + 객체 위치 정보 파악하는 위치 검출
- 1단계 객체 인식 : 두 문제를 동시에 행하는 방법 (YOLO, SSD, FcalLoss, RefineDet), 빠르지만 정확도 낮음
- 2단계 객체 인식 : 두 문제를 순차적으로 행함 (R-CNN, Fast R-CNN, Faster R-CNN), 느리지만 정확도 높음

-  R-CNN
    - 예전 : 슬라이딩 윈도우 방식(경계 상자를 만들어 탐색 과정 반복) -> 현재: 선택적 탐색 알고리즘 적용한 후보 영역 (region proposal)
    - 이미지 입력 받아 2000개 바운딩 박스를 선택적 탐색 알고리즘으로 추출 후 cropping(자르기), CNN 모델에 넣기 위해 (227x227)로 통일, 크기 동일 이미지 각각 적용 후 분류 진행
    - 단점 : 복잡한 학습 과정, 긴 학습 시간, 대용량 저장공간, 객체 검출 속도 문제

- 공간 피라미드 풀링
    - 신경망 통과 위해 crop 하거나 warp 해야하는데 본래와 달라져 문제를 해결하고자 도입
    - 이미지 크기 관계 없이 합성곱층 통과, 완전연결층에 도달 전 특성 맵과 동일 크기로 조절 해주는 풀링층 적용

- Fast R-CNN
    - 속도 개선 RoI 풀링 도입 : 크기가 다른 특성 맵의 영역마다 스트라이드 다르게 최대 풀링 적용해 결과값 크기 동일하게 함
- Faster R-CNN
    - 후보 영역 추출 네트워크 추가 : 객체 존재 유무 이진 분류 수행해 작은 네트워크 생성 => 다양한 크기 비율 이미지 수용 어렵다는 단점으로 여러 크기와 비율의 레퍼런스 박스 k개를 미리 정의해 각각 슬라이딩 윈도우 위치마다 박스 k개 출력해 설계 (앵커)
    - RPN 사용 : 마지막 합성곱층 다음에 ROI 풀링, 분류기, 바운딩 박스 회귀 위치

#### 이미지 분할
- 완전 합성곱 네트워크
    - 완전연결층의 한계는 고정된 크기 입력만 받기에 위치 정보가 사라진다는 단점 => 완전 연결층을 1x1 합성곱으로 대체
    - 장점 : 위치 정보 보존, 입력 크기 제약 없음
    - 단점 : 해상도 낮아짐, 복원 위해 업 샘플링 방식 사용하기에 이미지 세부 정보 잃어버림
- 합성곱&역합성곱 네트워크
    - 역합성곱 : 최종 출력 결과를 원래 입력 이미지와 같은 크기로 만들고 싶을 때 사용 (업 샘플링)
    - 각각 픽셀 주위에 제로 패딩 추가 후 패딩 된 것에 합성곱 연산 수행
- U-Net
    - 바이오 메디컬 이미지 분할을 위함
    - 속도 빠름, trade-off에 안빠짐 (지역화 개선)
    - FCN 기반으로 구축되었고, 수축경로(컨텍스트 포착)와 확장 경로(업 샘플링, 특성 맵 컨텍스트와 결합)로 구성
    - 각 합성곱 블록은 3x3 합성곱 + 드롭아웃 + 3x3 합성곱 => 최대 풀링 => up-conv 로 붙이고 => 3x3 합성곱 + 렐루
- PSPNet
    1. 이미지 출력 서로 다른 크기 되도록 여러 차례 풀링 => 서로 다른 영역 정보
    2. 1x1 합성곱 이용해 채널 수 조정 => 풀링층 개수 N일시 출력 채널 수 = 입력채널수/N
    3. 모듈 입력 크기에 맞게 특성 맵 업샘플링 (양선형 보간법)
    4. 원래 특성맵과 새로운 특성맵 병합 => 예측

- DeepLabv3
    - Atrous 합성곱 사용 : 빈공간 결정 파라미터 rate (r = 1 기존 합성곱과 동일 빈공간, r 커질 수록 빈 공간 수 많아짐) => 수용 영역 확대로 특성 찾는 범위 넓게 해줌, 기존 대비 4배 보존
    - 인코더와 디코더 구조