# 파이토치 공부
## 7장
### 공부 책 : 딥러닝 파이토치 교과서 (길벗)

시계열 형태 - 데이터 변동 유형에 따라 분규칙 변동, 추세변동, 순환변동, 계절 변동
1. 불규칙 변동 : 시간에 따른 규칙적 움직임과 달리 규칙성 없어 예측 불가능하고 우연적 발생
2. 추세 변동 : 장기적인 변화 추세. 짧은 기간 동안에는 추세 변동찾기 어려움
3. 순환 변동 : 2~3년 정도 일정 기간을 주기로 순환적으로 나타나는 변동
4. 계절 변동 : 계절적 영화과 사회적 관습에 따라 1년 주기로 발생

- AR 모델
    - 이전 관측 값이 이후 관측 값에 영향을 준다 (자기회귀 모델)
    - 현재 시점 = 과거가 현재에 미치는 영향 모수 * 과거 시점 + 오차항(백색 잡음)
- MA 모델
    - 트렌드 변화 상황에 적합
    - 윈도우 크기만큼 슬라이딩 (이동평균모델)
    - 현재 시점 = 매개 변수 * 과거 시점 오차 + 오차
- ARIMA 모델
    - 자기회귀와 이동 평균 둘다 고려
    - 과거 데이터 선형 관계 뿐만 아니라 추세까지 고려

### RNN
시간적으로 연속성 있는 데이터 처리 고안 인공 신경망
- 새로운 입력이 네트워크로 들어올 때마다 기억은 조금씩 수정되며 최종적으로 남겨진 기억은 모든 입력 전체 요약 정보
- 외부 입력과 자신의 이전 상태를 입력 받아 현재 상태를 갱신

1. 일대일 : 순환이 없기에 RNN 말하기 어려움. 순방향 네트워크
2. 일대다 : 입력이 하나이고 출력이 다수, 이미지 입력시 설명 문장 출력 이미지캡션
3. 다대일 : 입력이 다수 출력이 하나, 문장 입력해서 긍/부정 출력 감성 분석기

```python
nn.Embedding(len(TEXT.vocab.stoi), embeding_dim) #임베딩 처리
nn.RNNCell(input_dim, hidden_size) #RNN
nn.Linear(hidden_size, 256) #입력층
nn.Linear(256,3) #출력층
```
4. 다대다 : 언어를 번역 하는 자동 번역기
- 텐서플로는 간단하지만 파이토치는 시퀀스-투-시퀀스 이용
```python
Seq2Seq(
    encoder(
    embedding
    rnn
    dropout
    )
    decoder(
    embedding
    rnn
    fc_out
    dropout
    )
)
```
- RNN 셀은 RNN 계층의 for loop 구문을 갖는 구조 (RNN 계층은 순서대로 모두 처리, RNN 셀은 오직 하나의 단계만 처리)
- 입력층, 은닉층, 출력층, 가중치 (Wxh, Whh, Why)
    - Wxh : 입력층에서 은닉층, Whh : t 시점의 은닉층에서 t+1 시점 은닉층으로 전달 , Why : 은닉층에서 출력층
    - 가중치는 모든 시점에서 동일

1. 은닉층은 일반적으로 하이퍼볼릭 탄젠트 활성화 함수 사용
    - ht = tanh(y^t)
    - y^t = Whh * ht-1 + Wxh * xt
2. 출력층은 소프트 맥스 함수 사용
3. 오차는 각 단계마다 오차를 측정
4. 역전파는 BPTT 이용해 모든 단계마다 처음부터 끝까지 역전파 => 멀리 전파 될 수록 계산량 많아지고 양 적어져 기울기 소멸 문제 발생 =>
보완 위해 근본적으로 LSTM 이나 GRU 사용


### RNN 셀 구현
- torchtext는 NLP에서 사용하는 데이터로더 - 파일 가져오기, 토큰화 (넥스트를 문장이나 단어 분리), 단어 집합 (중복 제거 텍스트 총 단어 집합), 인코딩 (사람의 언어 문자를 컴퓨터 언어 숫자로), 단어 벡터 (단순히 단어 의미 나타내는 숫자 벡터)

```python
torchtext.legacy.data.Field(lower = True, fix_lenght = 200, batch_first = False)
# lower 는 소문자로 변경, 200으로 고정된 길이 데이터 얻음 (만일 짧을 시 패딩 작업으로 200 맞춰줌)
# 본래 네트워크 입력 데이터 (seq_len, batch_size, hidden_size) => True 면 (batch_size, seq_len, hidden_size)
torchtext.legacy.data.Field(sequential = False) #순서 여부

import string
text = [x.lower() for x in vars(example)['text']] # 소문자로 변경
text = [x.replace('<br','') for x in text] #'br'을 공백으로
text = [''.join(c for c in s if c not in string.punctuation) for s in text] # 구두점 제거
text = [s for s in text if s] # 공백 제거

text.build_vocab(train_data, max_size = 10000, min_freq = 10, vectors = None)
# 훈련 데이터 셋, 단어 집합 크기, 특정 단어 최소 등장 횟수, 임베딩 벡터 (word2vector, glove)

label.vocab.stoi # 데이터 셋 단어 집합
```

- 은닉층 층수는 비선형 문제를 좀 더 잘 학습 할 수 있도록 함
- 층 안에 있는 뉴런은 가중치와 바이어스 계산 용도

```python
nn.Embedding(len(text.vocab.stoi), embeding_dim) # 임베딩 할 단어 수 , 임베딩 할 벡터 차원

nn.CrossEntropyLoss() # 다중 분류에 사용, nn.LogSoftmax + nn.NLLLoss
torch.optim.Adam(model.parameters(), lr = 0.0001) # 파라미터 업데이트

for b in trainloader:
    x,y = b.text, b.label
    x,y = x.to(device),y.to(device)
    y_pred = model(x)
    loss = loss_fn(y_pred,y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    with torch.nograd():
        y_pred = torch.argmax(y_pred, dim = 1)
        correct += (y_pred == y).sum().item()
        total += y.size(0)
        running_loss += loss.item()

self.n_layers = n_layers #RNN 계층 개수
self.embed = nn.Embedding(n_vocab, embed_dim) # 워드임베딩 적용
self.hidden_dim = hidden_dim
self.dropout = nn.Droupout(droupout_p)
self.rnn = nn.RNN(embed_dim, self.hidden_dim, num_layers = self.n_layers, batch_first = True)
self.out = nn.Linear(self.hidden_dim, n_classes)

def forward(self, x):
    x = self.embed(x)
    h_0 = self.__init__state(batch_size = x.size(0)) #최초 은닉 상태를 0으로 초기화
    x, _ = self.rnn(x,h_0) # 파라미터로 입력과, 이전 은닉 상태 값 받음
    h_t = x[:,-1,:] #가장 마자막 나온 단어 임베딩 값
    self.dropout(h_t)
    logit = torch.sigmoid(self.out(h_t))
    return logit

def __init__state(self, batch_size = 1):
    weight = next(self.parameters()).data # weight 변수에 저장
    return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()
    # 계층 수, 배치 크기, 은닉층 뉴럿 수 인 은닉 상태 생성해 0으로 초기화
```
RNN은 가중치 업데이트 과정에서 기울기가 1보다 작은 값이 계속 곱해져서 기울기 소멸 문제 발생 => 해결 위해 LSTM, GRU 사용

### LSTM
- 순전파
    1. 망각게이트 - 과거 정보를 어느 정도 기억할지 결정 => 시그모이드 취한 후 과거 정보에 곱해 0이면 과거 정보 버리고 1이면 온전히 보존
    2. 입력게이트 - 현재 정보 기억 위해 생성 => 시그모이드와 하이퍼볼릭 탄젠트 함수 기반으로 현재 정보 보존량 결정 => 0이면 차단, 1이면 입력 허용
    3. 셀 - 각 단계에 대한 은닉노드, 총합을 사용해 셀 값 반영 후 이것이 기울기 소멸 문제 해결
        - 망각 게이트와 입력 게이트 이전 단계 셀 정보 계산해 셀 상태 업데이트
    4. 출력 게이트 - 이전 은닉 상태와 t번째 입력 고려해 다음 은닉 상태 계산 => LSTM에서는 이 은닉 상태가 그 시점의 출력
        - 1이면 의미 있는 결과, 0이면 출력 안함

- 역전파
    - 최종 오차는 모든 노드에 전파되는데 이때 셀 통해 중단 없이 전파

```python
self.x2h = nn.Linear(input_size, 4*hidden_size, bias = bias)
self.h2h = nn.Linear(hidden_size, 4*hidden_size, bias = bias)
# 4를 곱하는 이유는 입력, 망각, 셀, 출력 게이트에 쪼개지기 때문
gates = self.x2h(x) + self.h2h(hx)
gates = gates.squeeze() # 텐서의 차원을 줄이고자 할 때
ingate, forgetgate, cellgate, outgate = gates.chunk(4,1) #몇개로 쪼갤지, 어떤 차원을 기준으로 쪼갤지

cy = torch.mul(cx,forgetgate) + torch.mul(ingate, cellgate) #셀 상태
hy = torch.mul(outgate, F.tanh(cy)) #은닉 상태
#troch.mul([[3],[1]],3) = [[9],[3]]

self.lstm = LSTMCell(input_dim, hidden_dim, layer_dim)

# 은닉상태와 셀 상태를 0으로 초기화 => LSTM 셀 계층 반복 쌓음 (은닉상태와 셀 상태를 LSTMCell에 적용 결과를 또 다시 각 상태에 저장)
```
1. num_class, num_layer, input_size, hidden_size, seq_lenght를 입력 받기 
2. nn.LSTM, nn.Linear(hidden_size,128) - 완전 연결층 , nn.Linear(128,num_classes) - 출력층
3. 은닉 상태, 셀 상태 0으로 초기화, lstm 계층에 은닉상태와 셀 상태 적용
4. 은닉상태.view(-1, self.hidden_size) - 완전연결층 적용 위해 데이터 형태 조정
5. ReLU 각각 적용 후 완전연결층과 출력층 적용
6. torch.nn.MSELoss()와 Adam 설정
7. 에포크만큼 반복 (forward => zero_grad => backward => step)

### GRU
LSTM에서 망각 게이트와 입력 게이트를 합치고, 별도의 업데이트 게이트 구성

1. 망각 게이트 - 과거 정보 적당히 초기화 목적 시그모이드 함수와 출력 값을 이전 은닉층에 곱함 => 이전 시점 은닉층 값에 현 시점 정보 가중치 곱한 것과 같음
2. 업데이트 게이트 - 과거와 현재 정보 최신화 비율 결정
3. 후보군 - 현 시점 정보에 대한 후보군 계산 => 과거 은닉층 정보 그대로가 아닌 망각 게이트 결과 이용하여 후보군 계산
4. 은닉층 - 업데이트 게이트 결과와 후보군 결과 결합하여 현 시점 은닉층 계산

```python
self.x2h = nn.Linear(input_size, 3*hidden_size, bias = bias) #망각, 입력 게이트 + tanh 적용 new gate
self.h2h = nn.Linear(hidden_size, 3*hidden_size, bias = bias)
gate_x = self.x2x(x)
gate_h = self.h2h(hidden) # LSTM에서는 x2h+h2h 이지만 GRU에서는 개별

gate_x.chunk(3,1)
gate_h.chunk(3,1)
newgate = F.tanh(i + (resetgate*h))
hy = newgate + inputgate * (hidden-newgate)

#GRU에서는 셀 상태가 없다는 점!
```

### 양방향 LSTM
```python
self.lstm = nn.LSTM(input_size, hidden_size, num_layers, `bidirectional = True`, batch_first = True) # 양방향 LSTM 사용
self.fc = nn.Linear(`hidden_size*2`, num_classes) #양방향이기에 2배를 해줘야함
h_0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size))
c_0 = Variable(torch.zeros(self.num_layers*2, x.size(0), self.hidden_size))

```
- 나머지 처리 방식은 동일


