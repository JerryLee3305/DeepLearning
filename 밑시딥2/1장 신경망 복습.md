# 밑시딥 2
## 한빛 미디어
### 1장 신경망 복습

벡터 : 크기와 방향을 가진 양 - 파이썬에서는 1차원 배열로 취급

행렬 - 숫자가 2차원 형태로 늘어선 것

- 벡터의 내적 - np.dot()
- 행렬의 곱 - np.matmul()

행렬의 곱 등 행렬을 계산할 시 `형상 확인` 중요 (A 행 수와 B 열 수)

완전연결계층 - 인접하는 층의 모든 뉴런과 연결되어 있을 시

- h = xW + b
    - x는 입력, h는 은닉층의 뉴런, W는 가중치, b는 편향 (기호 모두 다 행렬)

완전연결계층에 의한 변환(Affine)은 `선형` 변환. `비선형` 효과 부여하는 것이 바로 `활성화 함수` - 비 선형 활성화 함수 : 시그모이드

```python
import numpy as np
class Sigmoid:
    def __init__(self):
        self.params = []
    
    def forward(self, x):
        return 1/ (1+np.exp(-x))

class Affine:
    def __init__(self, W,b):
        self.params = [W,b]
    
    def forward(self, x):
        W,b = self.params
        out = np.matmul(x,W) + b
        return out

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size):
        I,H,0 = input_size, hidden_size, output_size

        W1 = np.random.randn(I,H)
        b1 = np.random.randn(H)
        W2 = np.random.randn(H,0)
        b2 = np.random.randn(0)

        self.layers = [
            Affine(W1,b1),
            Sigmoid(),
            Affine(W2,b2)
        ]

        self.params = []
        for layer in self.layers:
            self.params += layers.params # 리스트 연산자 결합
        
    def predict(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x
```

- 학습 단계 특정 시점에서 신경망 성능 나타내는 척도 : `손실`
    - 다중 클래스 분류 시 Cross Entropy Error 사용
        - 미니배치 고려할 시 평균 손실 함수를 구함 => 미니배치 크기에 관계없이 항상 일관된 척도를 얻음

연쇄법칙 (chain rule) - 각 함수의 국소적인 미분을 계산할 수 있다면 그 값들을 곱해서 전체의 미분을 구할 수 있도록

- Repeat 노드
```python
D,B = 8,7
x = np.random.randn(1,D)
y = np.repeat(x,N,axis = 0) # 순전파
dy = np.random.randn(N,D)
dx = np.sum(dy, axis = 0, keepdims = True) #역전파
```

- Sum 노드
```python
D,B = 8,7
x = np.random.randn(N,D)
y = np.sum(x, axis = 0, keepdims = True) #역전파

dy = np.random.randn(1,D)
dx = np.repeat(dy,N,axis = 0) # 순전파
```

- MatMul 노드
```python
class MatMul:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.x = None
    
    def forward(self, x):
        W, = self.params
        out = np.matmul(x,W)
        self.x = x
        return out

    def backward(self,dout):
        W, = self.params
        dx = np.matmul(dout,W.T)
        dW = np.matmul(self.x.T, dout)
        self.grads[0][...] = dW # 깊은 복사로 덮여씌어짐
        return dx
```

- Sigmoid 계층
```python
class Sigmoid:
    def __init__(self):
        self.params, self.grads = [],[]
        self.out = None
    def forward(self,x):
        out = 1/(1+np.exp(-x))
        self.out = out
        return out

    def backward(self,dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```

- Affine 계층
```python
class Affine:
    def __init__(self, W,b):
        self.params = [W,b]
        self.grads = [np.zeros_like(W), np.zeros_like(b)]
        self.x = None
    
    def forward(self, x):
        W,b = self.params
        out = np.matmul(x,W) +b
        self.x = x
        return out
    
    def backward(self, dout):
        W,b = self.params
        dx = np.matmul(dout,W.T)
        dW = np.matmul(self.x.T, dout)
        db = np.sum(dout, axis = 0)

        self.grads[0][...] = dW
        self.grads[1][...] = db
        return dx
```

- 확률적경사하강법 (SGD)
```python
class SGD:
    def __init__(self, lr = 0.01):
        self.lr = lr
    def update(self, params, grads):
        for i in range(len(params)):
            params[i] -= self.lr * grads[i]

optimizer = SGD()
optimizer.update(model.params, model.grads)
```

#### 신경망 구현
```python
import sys
sys.path.append('..')
import numpy as np
from common.layers import Affine, Sigmoid, SoftmaxWithLoss

class TowLayerNet:
    def __init__(self, input_size, hidden_size, output_size):
        I,H,0 = input_size, hidden_size, output_size

        W1 = 0.01 * np.random.randn(I,H)
        b1 = np.zeros(H)
        W2 = 0.01 * np.random.randn(H,0)
        b2 = np.zeros(0)

        self.layers = [
            Affine(W1, b1),
            Sigmoid(),
            Affine(W2, b2)
        ]
        self.loss_layer = SoftmaxWithLoss()

        self.params, self.grads = [], []
        for layer in self.layers:
            self.params += layer.params
            self.grads += layer.grads

    def predict(self, x):
        for layer in self.layers:
            x = layer.forward(x)
    
    def forward(self,x,t):
        score = self.predict(x)
        loss = self.loss_layer.forward(score, t)
        return loss

    def backward(self, dout = 1):
        dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
            dout = layer.backward(dout)
        return dout
```

- 학습용 코드
```python
import sys
sys.path.append('..')
import numpy as np
from common.optimizer import SGD
from dataset import spiral
import matplotlib.pyplot as plt
from two_layer_net import TwoLayerNet

max_epoch = 300
batch_size = 30
hidden_size = 10
learning_rate = 1.0

x,t = spiral.load_data()
model = TwoLayerNet(input_size = 2, hidden_size = hidden_size, output_size = 3)
optimizer = SGD(lr = learning_rate)

data_size = len(x)
max_iters = data_size // batch_size
total_loss = 0
loss_count = 0
loss_list = []

for epoch in range(max_epoch):
    idx = np.random.permutation(data_size)
    x = x[idx]
    t = t[idx]

    for iters in range(max_iters):
        batch_x = x[iters * batch_size : (iters +1)*batch_size]
        batch_t = t[iters * batch_size : (iters+1) *batch_size]

        loss = model.forward(batch_x, batch_t)
        model.backward()
        optimizer.update(model.params, model.grads)

        total_loss += loss
        loss_count += 1

        if (iters +1) % 10 == 0:
            avg_loss = total_loss/loss_count
            loss_list.append(avg_loss)
            total_loss, loss_count = 0,0
```

- Trainer 클래스 이용해서 단순화 시키기
```python
from common.trainer import Trainer

model = TwoLayerNet(input_size = 2, hidden_size = hidden_size, output_size = 3)
optimizer = SGD(lr = learning_rate)

trainer = Trainer(model, optimizer)
trainer.fit(x,t,max_epoch, batch_size, eval_interval = 10)
trainer.plot()
```

#### 정리
- 신경망은 입력층, 은닉층(중간층), 출력층 가짐
- 완전연결계층에 의해 선형변환 (Affine), 활성화 함수에 의해 비선형 변환 (sigmoid)
- 완전연결계층이나 미니배치 처리는 행렬로 모아 한꺼번에 계산 가능
- 오차역전파법 사용해 신경망 손실 기울기 구할 수 있음
- 계층으로 모듈화하면 신경망 쉽게 구성
- GPU 이용시 병렬 계산과 데이터 비트 정밀도 중요
