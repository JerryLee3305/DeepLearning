# 밑시딥 2
## 한빛 미디어
### 2장 자연어와 단어의 분산 표현

자연어 : 우리가 평소에 쓰는 말

자연어 처리 (NLP) : 자연어를 처리하는 분야, 우리 말을 컴퓨터에게 이해시키기 위한 기술

#### 시소러스
- 시소러스 : 유의어 사전, 뜻이 같은 단어(동의어)
    - 자연어 처리에서는 사위와 하위 or 전체어 부분 등

- WordNet : 자연어 처리에서 가장 유명한 시소러스
    - 유의어 및 '단어 네트워크' 이용 가능 => 단어 사이 유사도 구할 수 있음

- 시소러스 문제점
    1. 시대 변화 대응 어려움 : 신조어, 언어 의미 변질
    2. 사람 쓰는 비용 큼 : 만드는 데 엄청난 인적 비용 발생
    3. 단어 미묘한 차이 표현 불가


#### 통계 기반
- 말뭉치 : 대량의 텍스트 데이터 (ex. 위키백과, 구글 뉴스)

말뭉치 전처리
```python
def preprocess(text):
    text = text.lower()
    text = text.replace('.', ' .')
    words = text.split(' ')
    word_to_id = {}
    id_to_word = {}

    for word in words:
        if word not in word_to_id:
            new_id = len(word_to_id)
            word_to_id[word] = new_id
            id_to_word[new_id] = word
    
    corpus = np.array([word_to_id[w] for w in words])

    return corpus, word_to_id, id_to_word

# 위와 같은 방식
from common.util import preprocess
corpus, word_to_id, id_to_word = preprocess(text)
```

- 단어의 의미를 정확하게 파악할 수 있는 벡터 표현 = `분산 표현`
- 분포 가설 : 단어의 의미는 주변 단어에 의해 형성 된다. (맥락)

- 동시발생 행렬
```python
def create_co_matrix(corpus, vocab_size, window_size = 1):
    corpus_size = len(corpus)
    co_matrix = np.zeros((vocab_size, vocab_size), dtype = np.int32)

    for idx, word_id = in enumerate(corpus):
        for i in range(1, window_size +1):
            left_idx = idx -i
            right_idx = idx + i

            if left_idx >= 0 :
                left_word_id = corpus[left_idx]
                co_matrix[word_id, left_word_id] +=1
            
            if right_idx < corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1
    
    return co_matrix
```

- 코사인 유사도 : 단어 벡터 유사도 나타낼 때 많이 사용
    - 벡터 방향이 완전히 같을 시 코사인 유사도 1, 완전히 반대라면 -1
```python
def cos_similarity(x,y):
    nx = x/np.sqrt(np.sum(x**2))
    ny = y/np.sqrt(np.sum(y**2))
    return np.dot(nx, ny)

# 위 방식을 사용할 시 제로 벡터가 들어온다면 0으로 나누는 오류가 발생
# 그렇기에 아래 방식을 사용하여 수정

def cos_similarity(x,y):
    nx = x/(np.sqrt(np.sum(x**2)) +eps)
    ny = y/(np.sqrt(np.sum(y**2)) +eps)
    return np.dot(nx, ny)

# 앱실론을 사용하면 기본값인 1e-8 로 설정됨

# 함수 사용하기
from common.util import preprocess, create_co_matrix, cos_similarity
# 전처리
corpus, word_to_id, id_to_word = preprocess(text)
# 동시발생 행렬 만들기
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)
#코사인 유사도 확인
c0 = C[word_to_id['You']] # You 의 단어 벡터
c1 = C[word_to_id['i']] # i 의 단어 벡터
cos_similarty(c0,c1) 

# 유사 단어 랭킹 만들기
def most_similar(query, word_to_id, id_to_word, word_matrix, top = 5):
    if query not in word_to_id:
        print('%s 를 찾을 수 없습니다.' % query)
        return
    
    print('/n[query]' + query)
    query_id = word_to_id[query]
    query_vec = word_matrix[query_id]

    vocab_size = len(id_to_word)
    similarity = np.zeors(vocab_size)
    for i in range(vocab_size):
        similarity[i] = cos_similarity(word_matrix[i], query_vec)

    count = 0
    for i in range(-1 * similarity).argsort(): # argsort()는 배열의 원소를 오름차순 정렬후 인덱스 반환 (원본의 인덱스)
        if id_to_word[i] == query:
            continue
        print(' %s: %s' % (id_to_word[i], similarity[i]))

        count +=1
        if count >= top:
            return

# 함수로 사용하기
from common.util import most_similar
most_similar('you', word_to_id, id_to_word, C, top = 5)
```

- 점별 상호정보량 (PMI) : 높을수록 관련성이 높다는 의미 => 단독 출현 횟수 고려해 자주 출현하면 점수가 낮아짐
    - 양의 상호정보량 (PPMI) : 동시발생 횟수가 0이면 -무한대가 됨 => 음수라면 -으로 취급
- ppmi 문제점 : 말 뭉치 어휘 수 증가함에 따라 단어 벡터 차원 수 증가, 원소 대부분 0, 노이즈에 약하고 견고하지 못하다는 단점 => 차원 축소를 진행
```python
def ppmi(C, verbos = False, eps = 1e-8):
    M = np.zeros_like(C, dtype = np.float32)
    N = np.sum(C)
    S = np.sum(C, axis = 0)
    totla = C.shape[0] * C.shape[1]
    cnt = 0

    for i in range(C.shape[0]):
        for j in range(C.shape[1]):
            pmi = np.log2(C[i,j] * N / (S[j]*S[i]) + eps) # pmi 구하는 공식, 0 방지를 위해 eps 추가
            M[i,j] = max(0, pmi)

            if verbose: #진행상황 출력 여부
                cnt +=1
                if cnt % (total //100 +1) == 0:
                    print(100*cnt/total)
    return M
```

- 차원 축소 : 벡터의 차원 줄이는 방법
    - 중요한 정보는 최대한 유지하면서 줄이는게 핵심
    - SVD (특이값 분해) : X = US(V.T)
        - U와 V는 직교행렬 (orthogonal), 서로 직교
        - S는 대각행렬 (diagonal) : 특이값 (singular value) 큰 순서대로 나열 - 해당 축의 중요도

```python
W = ppmi(C)
U,S,V = np.linalg.svd(W)
```

- 펜 트리뱅크 (PTB) 데이터 셋 : 주어진 기법의 품질을 측정하는 벤치마크로 주로 이용됨

#### 정리
- WordNet 등 시소러스 이용하면 유의어를 얻거나 단어 유사도 측정 등 유용한 작업 가능
- 시소러스 기반 기법은 만드는데 인적 자원 많이 들고 새로운 단어 대응에 어렵다는 단점
- 말뭉치 이용해 단어 벡터화 방식 주로 쓰임
- 단어 의미는 주변 단어에 의해 형성된다는 분포 가설에 기초
- 통계 기반 기법은 단어 주변 단어의 빈도를 집계 (동시발생 행렬)
- 동시발생 행렬을 PPMI 행렬 변환 후 다시 차원 감소시켜, 거대한 '희소벡터'를 작은 '밀집벡터'로 변환
- 단어 벡터 공간에서는 의미가 가까운 단어는 그 거리도 가까울 것으로 기대