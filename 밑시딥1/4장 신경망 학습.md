# 밑바닥부터 시작하는 딥러닝 1
## 한빛미디어
### 4장 신경망 학습

- 신경망 특징 : 데이터를 보고 학습할 수 있다는 점
- 사람이 생각한 특징 (SIFT, HOG) => 기계학습 (SVM, KNN) => 결과 도출
- 신경망은 이미지를 `있는 그대로` 학습

#### 손실 함수
- 손실 함수 : 신경망에서 하나의 지표를 기준으로 최적의 매개변수 값을 탐색
    - 일반적으로 오차제곱합과 교차 엔트로피 오차 사용

- 오차제곱합(sum of squared for error, SSE) : 가장 많이 쓰이는 손실 함수
    - 오차가 작아야 정답에 가까운 것으로 판단
```python
def sum_squared_error(y,t):
    return 0.5 * np.sum((y-t)**2)
```

- 교차 엔트로피 오차 (cross entropy error, CEE) : 정답과 가까울 수록 0에 다가가고 출력 1일 때 0이 됨.
```python
def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t*np.log(y+delta))
```
- log에 0을 입력시 -inf 가 되므로 -이 되지 않는 아주 작은 값인 1e-7을 입력해줘서 마이너스 무한대 발생 안되도록 함

- 미니배치 : 훈련 데이터로부터 일부만 골라 학습을 수행
    - np.random.choice(지정 범위, 원하는 개수) => 지정한 범위 수 중에서 무작위로 원하는 개수만 꺼냄

```python
def cross_entrop_error(y,t):
    if y.ndim ==1:
        t = t.reshape(1,t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y+1e-7))/batch_size # 원핫인코딩을 할 시

    return -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7)) / batch_size # 원핫인코딩 안할 시
```

- 신경망 학습 시 정확도를 지표로 삼으면 안됨! => 미분이 대부분의 장소에서 0이 되기 때문
    - 정확도는 매개변수 미소한 변화에는 거의 반응 안보이고 반응 있다면 비연속적으로 갑자기 변화
    - 손실 함수는 출력이 연속적으로 변하고 곡선의 기울기도 연속적으로 변함 => 미분이 어느 장소라도 0이 되지 않음

#### 수치 미분
- 반올림 오차 문제 발생 => 최종 계산 결과에 오차가 생기게 함
    - 개선 포인트로 미세한 값을 10^-4 로 표현 => 1e-4
- 임의 두 점 함수 값들의 차이 (차분)
    - x+h 와 x 사이 h를 무한히 0으로 좁히는 것이 불가능해 생기는 한계
    - x+h 와 x-h  으로 x 중심으로 전후 차분을 계산한다는 의미에서 중심 차분 혹은 중앙 차분이라 함.

- 편미분 : 변수가 여럿인 함수에 대한 미분

#### 기울기
- 기울기 : 모든 변수의 편미분을 벡터로 정리
- 경사법 : 기울기를 이용해 함수의 최솟값을 찾으려고 하는 것
    - 기울어진 방향으로 일정 거리만큼 이동후 기울기 구하고 방향으로 나아가기를 반복해 함수의 값을 점차 줄임
    - 학습률 (기호 : 에타) : 매개변수 값을 얼마나 갱신하느냐를 결정
        - 학습률이 너무 크면 큰 값으로 발산, 너무 작으면 갱신되지 않은 채로 끝나게 됨

```python
import sys, os
from common.functions import softmax, cross_entropy_error
from common.gredient import numerical_gradient

class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3)
    def predict(self,x):
        return np.dot(x, self.W)
    def loss(self,x,t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y,t)

        return loss
net = simpleNet()
net.predict(x) # 예측 수행
np.argmax(p) # 최댓값의 인덱스
net.loss(x,t) # 손실 함수 값

def f(W):
    return net.loss(x,t)
dW = numerical_gradient(f, net.W) 
# 람다 기법 사용하면 더 편리하게 작성 가능
f = lambda w: net.loss(x,t)
dW = numerical_gradient(f,net.W) # 이것을 통해 가중치 매개변수 갱신
```

#### 학습 알고리즘 구현
1. 미니배치 (훈련 데이터 중 일부 무작위 선별)
2. 각 가중치 매개변수 기울기 구해 손실 함수 값 가장 작게 하는 방향 제시
3. 가중치 매개변수를 기울기 방향으로 조금씩 갱신
4. 1~3 반복

- 확률적 경사 하강법 (SGD)
```python
# 2층 신경망 클래스
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b2'] = np.zeros(hidden_size)
    def predict(self,x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1']. self.params['b2']

        a1 = np.dot(x,W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(x,W2) + b2
        y = softmax(a2)
        return y
    
    def loss(self, x, t):
        y = self.predict(x)
        return cross_entropy_error(y,t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis = 1)
        t = np.argmax(t, axis = 1)
        accuracy = np.sum(y ==t) / float(x.shape[0])
        return accuracy
    
    def numerical_gradient(self,x,t): # 시간이 오래 걸리므로 시간 절약 위해 다음장에서 배울 gradient(self,x,t)
        loss_W = lambda W : self.loss(x,t)

        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads

network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size) # 미니배치
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grad = network.numerical_gradient(x_batch, t_batch) # 기울기 계산
    for key in ('W1', 'b1', 'W2','b2'): # 매개변수 갱신
        network.params[key] -= learning_rate * grad[key]
    loss = network.loss(x_batch, t_batch) # 학습 경과 기록
    train_loss_list.append(loss)

# 정확도 계산하기
iter_per_epoch = max(train_size/batch_size, 1)
if i % iter_per_epoch == 0:
    train_acc = network.accuracy(x_train, t_train)
    test_acc = network.accuracy(x_test, t_test)
    train_acc_list.append(train_acc)
    test_acc_list.append(test_acc)
```
- 훈련 데이터와 시험 데이터 평가 정확도에 차이가 없어야만 오버피팅이 일어나지 않는 것임
    - 오버피팅 발생시 시험 데이터에 대한 정확도가 점차 떨어지기 시작함 => 조기종료를 이용, 가중치 감소 및 드롭아웃 사용

#### 정리
- 신경망 학습은 손실 함수를 지표로 사용하고 값이 작아지는 방향으로 가중치 매개변수 갱신
- 가중치 매개변수 갱신 시 가중치 매개변수 기울기 이용해 기울어진 방향으로 가중치 값 갱신 작업 반복
- 아주 작은 값 주어졌을 때 차분으로 미분하는 것을 수치 미분이라 함
