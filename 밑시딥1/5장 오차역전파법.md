# 밑바닥부터 시작하는 딥러닝 1
## 한빛미디어
### 5장 오차역전파법

#### 계산 그래프
- 에지 : 노드 사이의 직선
- 계산 그래프 : 계산 과정을 노드와 화살표로 표현
    - 노드는 원으로 표기하고 안에 연산 내용 적음. 계산 결과를 에지 위제 적어 다음으로 이동하도록 함

    - 장점
        1. 국소적 계산 : 전체가 아무리 복잡해도 각 노드에서는 단순 계산에 집중해 문제 단순화 함
        2. 역전파를 통해 미분을 효율적으로 계산

#### 연쇄 법칙
- 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있음

#### 역전파
- 덧셈 노드 : 모두 1이 됨 => 그대로 다음 노드로 전달
- 곱셈 노드 : 서로 바뀐 값을 통해서 곱해 줌

#### 계층 구현
```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    def forward(self,x,y):
        self.x = x
        self.y = y
        out = x*y
        return out
    def backward(self, dout):
        dx = dout * self.y
        dy = dout * self.x
        return dx, dy

class AddLayer:
    def __init__(self):
        pass
    def forward(self, x, y):
        out = x+y
        return out
    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
```

#### 활성화 함수 계층 구현
- ReLU 계층
```python
class Relu:
    def __init__(self):
        self.mask = None
    def forward(self, x):
        self.mask = (x<=0) # 음수인 곳에 True를 반환해서 0으로 설정
        out = x.copy()
        out[self.mask] = 0
        return out
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        return dx
```

- Sigmoid 계층 : 1/(1+exp(-x))
1. '/' 노드 변환
2. '+' 노드 이므로 그대로 작성
3. 'exp' 노드에서는 exp(-x) 로 변환
4. 'x' 노드는 값을 서로 바꿔서 곱 (-1)을 곱해줌
```python
class Sigmoid:
    def __init__(self):
        self.out = None
    def forward(self,x):
        out = 1/(1+np.exp(-x))
        self.out = out
        return out
    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```

#### Affine/Softmax 계층 구현
- 어파인 변환 : 순전파 때 수행하는 행렬의 곱
    - 역전파 할 때는 Transpose를 이용
```python
class Affine:
    def __init__(self,W,b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.db = None
    def forward(self,x):
        self.x = x
        out = np.dot(x, self.W)+self.b
        return out
    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis = 0)
        return dx

```

- softmax-with-Loss
    - 클래스 개수가 3개면 softmax 계층 입력 3개

    - (y1,y2,y3) 는 softmax 계층의 출력이고 (t1,t2,t3) 는 정답 레이블 => (y1-t1, y2-t2, y3-t3) 역전파 결과면서 차분
```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None # softmax 출력
        self.t = None # 정답레이블 (원핫벡터)
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss
    def backward(self, dout = 1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t) / batch_size
        return dx
```

#### 오차역전파 구현
```python
self.layers = OrderedDict()
self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
self.layers['Relu1'] = Relu()
self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
self.lastLayer = SoftmaxWithLoss()

def predict(self,x):
    for layer in self.layers.values():
        x = layer.forward(x)
    return x

def gradient(self,x,t):
    self.loss(x,t)
    dout = 1
    dout = self.lastLayer.backward(dout)
    layers = list(self.layers.values())
    layers.reverse()
    for layer in layers:
        dout = layer.backward(dout)
```

#### 정리
- 계산 그래프의 노드는 국소적 계산 구성되며 조합해 전체 계산 구성
- 계산 그래프의 역전파로는 각 노드의 미분을 구할 수 있음
- 오차역전파법을 통해 기울기 효율적 계산
