# 밑바닥부터 시작하는 딥러닝 1
## 한빛미디어
### 6장 학습 관련 기술들

#### 매개변수 갱신
- 최적화 : 손실함수의 값을 가능한 낮추는 매개변수의 최적값 찾기

- SGD
```python
class SGD:
    def __init__(self, lr = 0.01):
        self.lr = lr # 학습률
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
optimizer = SGD()

for i in range(10000):
    x_batch, t_batch = get_mini_batch()
    grads = network.gradient(x_batch, t_batch)
    params = network.params
    optimizer.update(params, grads) # 최적화 시키기
```

- SGD 단점 : 비등방성 함수에서는 탐색 경로 비효율적 (기울기가 달라지는 함수)

- 모멘텀
    - x축의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속
    - y축의 힘은 크지만 위아래로 번갈아 받아서 상충해 속도는 안정적이지 않음
```python
class Momentum:
    def __init__(self, lr = 0.01, momentum = 0.9)
        self.lr = lr
        self.momentum = momentum
        self.v = None
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr * grads[key]
            params[key] += self.v[key]
```

- AdaGrad : 각각의 매개변수에 맞춤형 값을 만들어줘 개별 매개변수에 적응적으로 학습률 조정해 학습 진행
    - 최솟값을 향해 효율적으로 움직임
    - 기울기가 커서 처음엔 크게 움직이지만 움직임에 비례해 갱신 정도는 큰 폭으로 작아지도록 조정
```python
class AdaGrad:
    def __init__(self, lr = 0.01):
        self.lr = lr
        self.h = None
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key])+ 1e-7)
```

- Adam : 모멘텀과 AdaGrad 융합, 편향 보정

#### 가중치 초깃값
- 가중치 감소 기법 : 가중치 매개변수 값이 작아지도록 학습해 오버피팅 방지
    - 오버피팅을 억제해 범용 성능을 높임
- 가중치 초깃값을 0으로 설정 시 오차역전파법에서 모든 가중치 값이 똑같이 갱신됨 => 초깃값 무작위 설정하기

시그모이드 함수는 출력이 0에 가까워지거나 1에 가까워지면 미분이 0에 다가감. => 열전파 기울기 값 점점 작아지다 사라짐 = 기울기 소실

특정 값에 치우치면 다수의 뉴런이 거의 같은 값을 출력하고 있어 뉴런을 여러개 둔 의미가 없어짐 => 표현력을 제한

- Xavier 초깃값 : 광범위 분포 시킬 목적으로 가중치 적절한 분포 찾고 앞 계층의 노드가 n개라면 표주편차가 1/루트n 인 분포 사용
    - 앞 층 노드 많을수록 대상 노드 초깃값으로 설정하는 가중치 좁아짐
    - 활성화 함수가 선형인 것을 전제
```python
node_num = 100 # 앞 층 노드 수
W = np.random.randn(node_num, node_num) / np.sqrt(node_num)
```

- ReLU 이용시 ReLU에 특화된 초깃값 이용 권장 => He 초깃값
    - 앞 계층 노드가 n개 일때 표준편차 루트(2/n) 정규분포 사용
    - 층이 깊어져도 분포가 균일하게 유지되기에 역전파 할 때도 적절값 나옴

#### 배치 정규화
- 각 층 활성화를 적당히 퍼뜨리도록 강제시키도록
- 학습 빨리 진행, 초깃값 크게 의존하지 않음, 오버피팅 억제

미니 배치를 단위로 정규화 (분포가 평균이 0, 분산 1)

#### 바른 학습
- 오버피팅 발생 경우 : 매개변수 많고 표현력 높은 모델, 훈련 데이터가 적을 때
    - 훈련 때 사용하지 않은 시험 데이터에는 제대로 대응하지 못함

- 오버피팅 억제 위해 가중치 감소 사용
    - 모든 가중치 각각의 손실 함수에 1/2 * 람다 * W^2 더해줌 => 가중치 기울기 구하는 계산에서는 결과에 람다 *W 를 더해줌

- 드롭 아웃 : 오버피팅 억제
    - 뉴런을 임의로 삭제하면서 학습
    - 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택 후 시험 때는 모든 뉴런에 신호 전달 (단 이때 뉴런 출력에 훈련 때 삭제 안한 비율을 곱해서 출력)

```python
class Dropout:
    def __init__(self, dropout_ratio = 0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None
    
    def forward(self, x, train_flg = True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x*self.mask
        else:
            return x * (1.0 - self.dropout_ratio)
    def backward(self, dout):
        return dout * self.mask
```

#### 적절한 하이퍼파라미터 값 찾기
하이퍼 파라미터 성능 평가할 때 시험 데이터 사용해서는 안됨 => 오버피팅이 되기 때문

하이퍼 파라미터 조정용 데이터를 일반적으로 검증 데이터라 부름

- 대략적 범위를 설정 후 범위에서 무작위로 하이퍼파라미터 값 샘플링 후 정확도 평가하고 반복하여 최적의 값 범위 좁혀가기


#### 정리
- 매개변수 갱신 방법에는 SGD, 모멘텀, AdaGrad, Adam 등이 있다
- 가중치 초깃값으로는 'Xaiver 초깃값', 'He 초깃값' 이 효과적
- 배치정규화 이용 시 학습 빠르게 진행하고 초깃값 영향 덜 받음
- 오버피팅 억제 위해서 가중치 감소와 드롭아웃이 있다.
- 하이퍼파라미터 값 탐색 시 최적 값 존재할 법한 범위 좁히면서 하는 것이 효과적